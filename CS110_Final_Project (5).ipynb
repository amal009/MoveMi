{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS110 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oV2V2zWtlX",
        "colab_type": "text"
      },
      "source": [
        "#MoveMi : CS110 Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPs2DopXy_f",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "\n",
        "###Prompt\n",
        "\n",
        "This project addresses Prompt 1: using data from a publicly available dataset, build an application that uses concepts from CS110. \n",
        "\n",
        "###Purpose\n",
        "\n",
        "####Content Disovery is broken in the age of Social media \n",
        "\n",
        "Content discovery and self-expression are hampered by the typical use of platforms today. \n",
        "\n",
        "Firstly, content consumptions is changing rapidly among teens. Time spent reading and even on traditional media is rapidly declining, and teens are instead spending large amounts of time on social media. The average teenager spends just 7 minutes reading a day, but spend 2.5 hours a day on social media and another 2 hours on television/youtube. \n",
        "\n",
        "One reason why reading has declined so rapidly is because social media and search engines do not  not support content discovery of written works effectively. Content created on the web tend to be on centralized platforms such as Reddit/Facebook/Youtube instead of smaller websites/blogs. \n",
        "\n",
        "This gives rise to 2 modes of content discovery. \n",
        "\n",
        "In the first mode of content discovery, through search engines such as google, the user needs to know exactly what he wants and can find it. However, this leads to the vast majority of content ends up undiscovered, even when it could add great value to the end user's life. Search engines aid discovery of the known, but not exploration of the unknown, which greatly limits the scope of content users discover.  \n",
        "\n",
        "In the second mode of content discovery, through social networks, individuals discover content through their friends and share content with them. This expands the scope of content discovery relative to search engines, to incorporate exploration of the unknown. However, the social element leads to a lack of sharing. Individuals are less likely to voice out their opinions due to a fear of offending as well as a assumption that not many people would share their interests. Because all interactions on social media are not anonymous, fear of social judgement also has an effect, which reduces the extent to which individuals respond to content through comments, which further reinforces the lack of social attention shown to content posted.\n",
        "\n",
        "####Self-expression is therefore not widespread,  genuine or fulfilling \n",
        "\n",
        "In general, these factors lead to content being posted on social media becoming less in-depth and specialized, and more attention-seeking and mainstream - in genral the shift is away from content articles and instead towards photos, selfies and the like. \n",
        "\n",
        "\n",
        "This, in turn, affects self-expression. More validation is provided for mainstream, attention-seeking posts, and social validation is a powerful incentive, especially among youth because they are continuing to form a self-identity. Therefore, youth tend to express themselves through self-validating posts such as photos, instead of through the written word which takes more effort but provides less validation. Moreover,with a lack of access to content that they find relatable and accessible, many individuals simply do not read enough, and do not know how to self-express \n",
        "\n",
        "####Solution: Content Discovery through anonymous self-expression \n",
        "\n",
        "The purpose of this algorithm is to implement a third model of content discovery - discovery of content through anonymous self-expression in the form of writing, similar to diary-writing. Individuals are aided and urged to write a entry where they are vulnerable and express their feelings (anonymously). Their diary entry is analyzed and the users are returned  relatable content that will touch them and add value to their lives. \n",
        "\n",
        "The purpose of this is 2-fold. Firstly, it will be able to provide content that is much more valuable to the end user. While initially the content served to the end user is either through his/her social network or through an intentional search, both of which are not effective when the end user is unclear what he is searching for in the first place , now the content is exactly tailored to the end user's emotional state or life circumstances, and is able to enrich and add value to his/her life. A person who expresses his insecurity over his exam grades, for example, would get directed to posts that are also about exam grades which mostly closely match his own. \n",
        "\n",
        "Second, it will make genuine self-expression much easier. While previously, genuine, writing-baseed self-expression was done in isolation due to the lack of social validation. Now there is a third, more -powerful incentive to self-express - discovering valuable content that will touch and enrich the end user. \n",
        "\n",
        "The content scope is limited to diary entries or other forms of emotionally invested medium-to-long forms of writing, in order to be best able to provide content that the end user will find moving. \n",
        "\n",
        "####Overview of Process \n",
        "The algorithm has 2 broad processes - indexing and retrieval. Every diary entry goes through both of these processes. \n",
        "\n",
        "In indexing, the  is analyzed and stored so that it can be easily discovered if it is deemed relevant to a later query. The diary entry is run through Google's Natural Language processing API to discover the content category as well as the main entities present in the entry, and using hash tables, search trees and heaps, this content is indexed. (This process will be explained in greater detail later)\n",
        "\n",
        "In retrieval, the diary entry's content categories and entities are analyzed and other diary entries that have the most similar content categories and entities are returned. </br> \n",
        "\n",
        "\n",
        "This paper will first outline the overall process and the high-level view of the data structures and algorithms used in designing this app, and the reasons behind their choice. Second, the overall algorithm will be explained, Third, the details of the implementations of heaps, hash tables and trees will be elaborated on. Lastly, the overall algorithm performance and complexity will be analyzed. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtZYKNhychuH",
        "colab_type": "text"
      },
      "source": [
        "##Data structures \n",
        "\n",
        "***Story Array*** <br>\n",
        "Purpose: An array containing all stories <br><br>\n",
        "\n",
        "\n",
        "***Category Tree ***\n",
        "\n",
        "Purpose: This is a hash table that stores Category Objects.<br>\n",
        "Subsumed Under: None <br>\n",
        "Data: [Category Object ] x n, where n  = length of CategoryTree hash table<br>\n",
        "<br>\n",
        "\n",
        "***Category Object***\n",
        "\n",
        "Purpose: This represents one category, and stores stories that are classified under that category <br>\n",
        "Subsumed Under: Category Tree Data Structure<br>\n",
        "Data: <br>\n",
        "ParentCategory - Index of Parent CategoryObjects<br>\n",
        "ChildrenCategories - Array of indices of children CategoryObjects <br>\n",
        "Entities Array - Unique array of Entity Objects \n",
        "\n",
        "<br>\n",
        "***Entity Object*** \n",
        "\n",
        "Purpose: This is a max-heap that is present for each entity in each category. <br>\n",
        "Subsumed Under: Category Object<br>\n",
        "Data: <br>\n",
        "Entity Max Heap, represented as an array. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbUEUFGRkvTm",
        "colab_type": "text"
      },
      "source": [
        "###Algorithm Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sksaFPSokzf3",
        "colab_type": "text"
      },
      "source": [
        "**Pre-requisites** <br> 1) Relevant libraries are installed <br>2) Output.txt inserted in local directory <br> 3) Google Natural Language API is activated in Google Cloud Console "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49dcr6uOlBMb",
        "colab_type": "code",
        "outputId": "e7f9285d-374e-421f-f990-8afbba93b4df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install --upgrade google-api-python-client\n",
        "!pip install pyhash\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import pyhash\n",
        "lservice = build('language', 'v1', developerKey='INSERT KEY HERE')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-api-python-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/47/940908e52487440f61fb93ad55cbbe3a28235d3bb143b26affb17b37dd28/google_api_python_client-1.7.7-py2.py3-none-any.whl (56kB)\n",
            "\r\u001b[K    18% |█████▉                          | 10kB 9.9MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 20kB 1.5MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▍              | 30kB 1.8MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▏        | 40kB 1.6MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 51kB 1.9MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.0.3)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (0.11.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.3)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.5)\n",
            "Installing collected packages: google-api-python-client\n",
            "  Found existing installation: google-api-python-client 1.6.7\n",
            "    Uninstalling google-api-python-client-1.6.7:\n",
            "      Successfully uninstalled google-api-python-client-1.6.7\n",
            "Successfully installed google-api-python-client-1.7.7\n",
            "Collecting pyhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/61/56af3d9ab410b0f73a1a38cf8a93f656ee9937d59c7074a3afa5edbd6008/pyhash-0.9.1.tar.gz (594kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 3.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyhash\n",
            "  Running setup.py bdist_wheel for pyhash ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/0a/dd/92/06521a3888c2b46df4a189f3458a78de4d769764f2a5ec639d\n",
            "Successfully built pyhash\n",
            "Installing collected packages: pyhash\n",
            "Successfully installed pyhash-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_52MdcqkmSVk",
        "colab_type": "text"
      },
      "source": [
        "####Step 1: Initialize Classes\n",
        "\n",
        "There are 2 classes to be initialized - maxHeap and category. These are explained above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y8g6LpYmpCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class maxHeap:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.salienceValues = []\n",
        "    \n",
        "    \n",
        "  def push(self, data):\n",
        "    self.salienceValues.append(data)\n",
        "    self.floatUp(len(self.salienceValues) - 1)\n",
        "    \n",
        "  def getMax(self): \n",
        "    return self.salienceValues[0]\n",
        "  \n",
        "  def delete(self,data):\n",
        "    index = self.salienceValues.index(data)\n",
        "    \n",
        "    if len(self.salienceValues) > 2:\n",
        "      self.swap(index, len(self.salienceValues) - 1)\n",
        "      returnValue = self.salienceValues.pop()\n",
        "      self.bubbleDown(index)\n",
        "    elif len(self.salienceValues) <= 2:\n",
        "      returnValue = self.salienceValues.pop()\n",
        "    return returnValue\n",
        "\n",
        "  def swap(self, i, j):\n",
        "    self.salienceValues[i], self.salienceValues[j] = self.salienceValues[j], self.salienceValues[i]\n",
        "\n",
        "  def floatUp(self, index):\n",
        "    parent = index//2\n",
        "    arrayInput = self.salienceValues\n",
        "    \n",
        "    if index <= 0: \n",
        "      return\n",
        "    elif arrayInput[index][\"salienceValue\"] > arrayInput[parent][\"salienceValue\"]:\n",
        "      self.swap(index, parent)\n",
        "      self.floatUp(parent)\n",
        "\n",
        "  def bubbleDown(self, index):\n",
        "    left = index * 2\n",
        "    right = index * 2 + 1\n",
        "    largest = index\n",
        "    if len(self.salienceValues) > left and self.salienceValues[largest][\"salienceValue\"] < self.salienceValues[left][\"salienceValue\"]:\n",
        "      largest = left\n",
        "    if len(self.salienceValues) > right and self.salienceValues[largest][\"salienceValue\"] < self.salienceValues[right][\"salienceValue\"]:\n",
        "      largest = right\n",
        "    if largest != index:\n",
        "      self.swap(index, largest)\n",
        "      self.bubbleDown(largest)\n",
        "\n",
        "class storyCategory :\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.parent = None \n",
        "        self.children = []\n",
        "        self.entities = [None]*500\n",
        "    \n",
        "    def findParent(self):\n",
        "      return self.parent\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3buYXthyZr2",
        "colab_type": "text"
      },
      "source": [
        "####Step 2:  Define the Hash Function\n",
        "\n",
        "The Hash function will be used for <br> 1)Hashing the category name to decide which position in the CategoryTree array to insert the story in <br> 2)Hashing the entity to decide which position in the Entities array, present in each Category Object, to  insert the story in\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpSqb_W6yYEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hashFunction(item, arrayLength,seed):\n",
        "  fp = pyhash.fnv1_32(seed)\n",
        "  return (fp(item) % arrayLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5lpvAOjkngZ",
        "colab_type": "text"
      },
      "source": [
        "####Step 2: Storage of Story in StoryArray\n",
        "\n",
        "The post is stored in an array, and its a dictionary containing the story and its index is returned . This was conducted in the storeStory() function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mbhKWhQlZDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def storeStory(story,storyArray): \n",
        "  #In this function,the quote is stored in an array, and the index of the array is attached to the quote \n",
        "\n",
        "  storyArray.append(story)\n",
        "  storyIndex = storyArray.index(story)\n",
        "  \n",
        "  storyDict = {}\n",
        "  storyDict[\"story\"] = story\n",
        "  storyDict[\"index\"] = storyIndex \n",
        "  return storyArray,storyDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAUIjxp8ksD5",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3: Analyze story using Google Natural Language API to get story's content category and entities  \n",
        "The post is fed through the Google Natural Language AI to find out what content category it is in, and what entities it has. This is stored in a storyObject. This was conducted in the classify() function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVku4eIRmDzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(storyDict): \n",
        "\n",
        "  #This function takes in a quote, runs it through the Google Natural Language API and returns the category and entities \n",
        "  story = storyDict[\"story\"]\n",
        "  StoryEntitiesArray = []\n",
        "  \n",
        "  entitiesResponse = lservice.documents().analyzeEntities(\n",
        "    body={\n",
        "      'document': {\n",
        "         'type': 'PLAIN_TEXT',\n",
        "         'content': story\n",
        "      }\n",
        "    }).execute()\n",
        "  try: \n",
        "    entities = entitiesResponse['entities']\n",
        "  except: \n",
        "    entities = [\"None\"]\n",
        "  for i in range(0,len(entities)): \n",
        "    entityName = entities[i]['name']\n",
        "    entitySalience = entities[i]['salience']\n",
        "    entityIndex = storyDict[\"index\"]\n",
        "    entitiesDictionaryEntry = {}\n",
        "    entitiesDictionaryEntry[\"Name\"] = entityName\n",
        "    entitiesDictionaryEntry[\"salienceValue\"] = entitySalience\n",
        "    entitiesDictionaryEntry[\"index\"] = entityIndex\n",
        "    \n",
        "    StoryEntitiesArray.append(entitiesDictionaryEntry)\n",
        "\n",
        "  response = lservice.documents().classifyText(\n",
        "    body={\n",
        "      'document': {\n",
        "         'type': 'PLAIN_TEXT',\n",
        "         'content': story\n",
        "      }\n",
        "    }).execute()\n",
        "  try: \n",
        "    category = response['categories'][0]['name'] \n",
        "  except: \n",
        "    category = \"None\"\n",
        "  \n",
        "  overallDictionary = {}\n",
        "  overallDictionary[\"Story\"] = story\n",
        "  overallDictionary[\"Category\"] = category\n",
        "  overallDictionary[\"Entities\"] = StoryEntitiesArray\n",
        "  overallDictionary[\"Index\"] = storyDict[\"index\"]\n",
        "  return overallDictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X0Meji_nEE6",
        "colab_type": "text"
      },
      "source": [
        "####Step 4: Store content category in Category tree, initialize relationships \n",
        "\n",
        "The content category is run through a hash function, and in the corresponding index in the hash array, a subcategory object with attributes 𝑝 for parent subcategory , 𝑐 for children subcategory and E for entity hashtable are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7XsAc_Hne32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initializeRelationships(categoryTree,storyObject): \n",
        "  categoryTotal = storyObject[\"Category\"]\n",
        "  final = []\n",
        "  currentString = \"\"  \n",
        "  for i in range(1,len(categoryTotal)): \n",
        "    if(categoryTotal[i]) == '/': \n",
        "      final.append(currentString)\n",
        "      currentString = \"\"\n",
        "    else: \n",
        "      currentString += categoryTotal[i]\n",
        "  final.append(currentString)\n",
        "  \n",
        "  hashSeed = 3\n",
        "  insertPosition = hashFunction(final[-1],len(categoryTree),3)\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  Open addressing is implemented \n",
        "  '''\n",
        "  if categoryTree[insertPosition] == None: \n",
        "    categoryTree[insertPosition] = storyCategory(final[-1])\n",
        "    \n",
        "    for i in range(1,len(final)): \n",
        "      parentPosition = hashFunction(final[-i-1],len(categoryTree),3)\n",
        "      categoryTree[parentPosition] = storyCategory(final[-i-1])\n",
        "      categoryTree[parentPosition].children.append(hashFunction(final[-i],len(categoryTree),3))\n",
        "\n",
        "      childPosition = hashFunction(final[-i],len(categoryTree),3)\n",
        "      categoryTree[childPosition].parent = hashFunction(final[-i-1],len(categoryTree),3)\n",
        "      \n",
        "  while categoryTree[insertPosition].name != final[-1]: \n",
        "    hashSeed += 1 \n",
        "    insertPosition = hashFunction(final[-1],len(categoryTree),hashSeed)\n",
        "    if categoryTree[insertPosition] == None: \n",
        "      categoryTree[insertPosition] = storyCategory(final[-1])\n",
        "\n",
        "      for i in range(1,len(final)): \n",
        "        parentPosition = hashFunction(final[-i-1],len(categoryTree),3)\n",
        "        categoryTree[parentPosition] = storyCategory(final[-i-1])\n",
        "        categoryTree[parentPosition].children.append(hashFunction(final[-i],len(categoryTree),3))\n",
        "\n",
        "        childPosition = hashFunction(final[-i],len(categoryTree),3)\n",
        "        categoryTree[childPosition].parent = hashFunction(final[-i-1],len(categoryTree),3)\n",
        "\n",
        "  \n",
        "  \n",
        "    \n",
        "    \n",
        "  #if categoryTree[insertPosition] is some other guy, then hash again with a new seed, to find a new insertPosition\n",
        "  \n",
        "  return categoryTree[insertPosition]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmcxGuIElj6P",
        "colab_type": "text"
      },
      "source": [
        "####Step 4: Store content category in Category tree, initialize relationships \n",
        "The entities of the post are run through a hash function. In the corresponding index in the entity hashtable E, which contains a max heap, the node at the root of the max-heap, which has the highest salience, is appended to returnStoryPointers, an array of the relevant stories that will be returned to the user. \n",
        "\n",
        "Then, the story itself is appended each of the max heaps which correspond to its entities \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx1IYqpln9dW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def insertStoryIntoTree(newCategory,categoryTree,storyObject): \n",
        "  \n",
        "  '''\n",
        "  This variables stores the pointers to the stories in the storyArray that are \n",
        "  most relevant \n",
        "  '''\n",
        "  returnStoryPointers = []\n",
        "  \n",
        "  '''\n",
        "  For each entity in the story being analyzed, first the category it is\n",
        "  classified under is searched for stories that share the same entities and\n",
        "  are hence relevant \n",
        "  '''\n",
        "  for i in range(0,len(storyObject[\"Entities\"])):  \n",
        "    entity = storyObject[\"Entities\"][i]\n",
        "    entityName = entity[\"Name\"]\n",
        "    entitySalience = entity[\"salienceValue\"]\n",
        "    entityIndex = entity[\"index\"]\n",
        "    storageIndex = hashFunction(entityName,len(storyObject[\"Entities\"]),3)\n",
        "    \n",
        "    if (newCategory.entities[storageIndex] != None):  ##AND whatever is in entities is the same as what is being queried \n",
        "      index = newCategory.entities[storageIndex].getMax()[\"index\"]\n",
        "      if (index not in returnStoryPointers) and (index != storyObject[\"Entities\"][0][\"index\"]) :\n",
        "        returnStoryPointers.append(index)\n",
        "      \n",
        "      '''\n",
        "      #This optional piece of code expands the functionality such that instead \n",
        "      #of returning the top 1 story per entity, it returns the top n stories, \n",
        "      #with n defined by the variable numberPerEntity. It is not activated \n",
        "      #because of the insufficient number of stories so far. \n",
        "      \n",
        "      poppedStoriesStorageArray = []\n",
        "      numberPerEntity = 3 \n",
        "      for i in range(0,numberPerEntity): \n",
        "        poppedStoriesStorageArray.append(newCategory.entities[storageIndex].pop(newCategory.entities[storageIndex].getMax()))\n",
        "        returnStoryPointers.append(newCategory.entities[storageIndex].getMax())\n",
        "        \n",
        "      for i in range(0,len(poppedStoriesStorageArray)): \n",
        "        newCategory.entities[storageIndex].push(poppedStoriesStorageArray[i])\n",
        "      \n",
        "      '''\n",
        "    #else if not equal none but not same as being queried, hash with a different seed. \n",
        "    \n",
        "    \n",
        "    else:\n",
        "      newCategoryMaxHeap = maxHeap()\n",
        "      newCategory.entities[storageIndex] = newCategoryMaxHeap\n",
        "    \n",
        "    '''\n",
        "    After exploring the category for similar stories, the story being analyzed\n",
        "    is inserted into the tree \n",
        "    '''\n",
        "    newCategory.entities[storageIndex].push(entity)\n",
        "  return returnStoryPointers\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0eh2QXDtL5E",
        "colab_type": "text"
      },
      "source": [
        "#### Step 5: Searching of parent and sibling categories\n",
        "\n",
        "If the number of stories returned from searching for entities at a particular category is less than 15, then the parent and sibling categories will also be searched for the same entities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY_auHA0talT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def searchParentSiblingCategories(returnStoryPointers,newCategory,categoryTree,storyObject):\n",
        "  '''\n",
        "  If there are less than 15 stories returned, the parent and sibling categories\n",
        "  are also explored for stories that share the same entities\n",
        "  '''\n",
        "  familyCategories = []\n",
        "      \n",
        "  if newCategory.parent != None: \n",
        "    parentCategory = categoryTree[newCategory.parent]\n",
        "    familyCategories.append(parentCategory)\n",
        "      \n",
        "    siblingCategories = parentCategory.children\n",
        "    for child in siblingCategories: \n",
        "      if categoryTree[child] != newCategory: \n",
        "        siblingCategory =  categoryTree[child] \n",
        "        familyCategories.append(siblingCategory) \n",
        "      \n",
        "  for entity in storyObject[\"Entities\"]:\n",
        "    entityName = entity[\"Name\"]\n",
        "    entitySalience = entity[\"salienceValue\"]\n",
        "    storageIndex = hashFunction(entityName,len(storyObject[\"Entities\"]),3)\n",
        "\n",
        "         \n",
        "    if (newCategory.entities[storageIndex] != None):\n",
        "      for familyCategory in familyCategories: \n",
        "        if familyCategory.entities[storageIndex] != None: \n",
        "          index = familyCategory.entities[storageIndex].getMax()[\"index\"]\n",
        "          if (index not in returnStoryPointers) and (index != storyObject[\"Entities\"][0][\"index\"]) :\n",
        "            returnStoryPointers.append(index)\n",
        "          \n",
        "  return returnStoryPointers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwvWsbbDtwz7",
        "colab_type": "text"
      },
      "source": [
        "##Test case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8PnAPxt0td",
        "colab_type": "text"
      },
      "source": [
        "####Data Scraping from Humans of New York\n",
        "This idea was heavily inspired by the form and popularity of the \"Humans of New York\" facebook page, which posts photojournalist entries where individuals are photographs and tell a story. The stories tend to be real, authentic and cross geographical, cultural and economic boundaries, and the page has exploded in popularity. Therefore, the dataset used was obtained from the Humans of New York facebook page using a custom-built web scraper that ran on Selenium, ActionDriver and BeautifulSoup Libraries. This program needs to be locally run, with the dependencies installed, because it initiates, controls and tracks a new web browser object (Firefox), which cannot be done via an online python notebook environment. 84 stories were downloaded to give a sample dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHUCoyBKuy5m",
        "colab_type": "code",
        "outputId": "925645be-2da8-45fa-d029-4271f58a6cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#This code is commented out in order to prevent interferance with the \"Run all\"\n",
        "#function in Python notebooks, as it is meant to be be run locally \n",
        "\n",
        "from selenium import webdriver\n",
        "import time\n",
        "import csv \n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "driver = webdriver.Firefox(executable_path = '/usr/local/bin/geckodriver')\n",
        "driver.get('https://www.facebook.com/pg/humansofnewyork/photos/?ref=page_internal')\n",
        "x = input(\"PROCEED?\")\n",
        "urlStorageArray = []\n",
        "storyStorageArray = []\n",
        "\n",
        "storyRange = 3\n",
        "#Gets all urls\n",
        "for i in range(storyRange):\n",
        "    print(\"FRACTION DONE:\",i,\"/\",storyRange)\n",
        "    time.sleep(3)\n",
        "    print(\"NEXT\")\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") \n",
        "\n",
        "a = driver.find_elements_by_css_selector('a')\n",
        "\n",
        "\n",
        "for element in a:\n",
        "    string = element.get_attribute('href')\n",
        "    if \"photos/a\" in string:\n",
        "        urlStorageArray.append(string)\n",
        "\n",
        "\n",
        "text_file = open(\"Output.txt\", \"w\", encoding='utf-8')\n",
        "for i in range(0,len(urlStorageArray)):\n",
        "    print(\"COMPLETED: \", i, \"/\", len(urlStorageArray)) \n",
        "    item = urlStorageArray[i]\n",
        "    driver.get(item)\n",
        "    element = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div[2]/div[2]/div[2]/div[2]/div/div[1]/div/div/div/div/div[3]/div[1]/div[2]/div[2]/p')\n",
        "    storyStorageArray.append(element.text)\n",
        "    text_file.write(storyStorageArray[i] + '\\n')\n",
        "    \n",
        "text_file.close()\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#This code is commented out in order to prevent interferance with the \"Run all\"\\n#function in Python notebooks, as it is meant to be be run locally \\n\\nfrom selenium import webdriver\\nimport time\\nimport csv \\nfrom selenium.webdriver.support.ui import WebDriverWait\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.common.action_chains import ActionChains\\nfrom selenium.webdriver.common.keys import Keys\\n\\ndriver = webdriver.Firefox(executable_path = \\'/usr/local/bin/geckodriver\\')\\ndriver.get(\\'https://www.facebook.com/pg/humansofnewyork/photos/?ref=page_internal\\')\\nx = input(\"PROCEED?\")\\nurlStorageArray = []\\nstoryStorageArray = []\\n\\nstoryRange = 3\\n#Gets all urls\\nfor i in range(storyRange):\\n    print(\"FRACTION DONE:\",i,\"/\",storyRange)\\n    time.sleep(3)\\n    print(\"NEXT\")\\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") \\n\\na = driver.find_elements_by_css_selector(\\'a\\')\\n\\n\\nfor element in a:\\n    string = element.get_attribute(\\'href\\')\\n    if \"photos/a\" in string:\\n        urlStorageArray.append(string)\\n\\n\\ntext_file = open(\"Output.txt\", \"w\", encoding=\\'utf-8\\')\\nfor i in range(0,len(urlStorageArray)):\\n    print(\"COMPLETED: \", i, \"/\", len(urlStorageArray)) \\n    item = urlStorageArray[i]\\n    driver.get(item)\\n    element = driver.find_element_by_xpath(\\'/html/body/div[1]/div[3]/div[1]/div/div[2]/div[2]/div[2]/div[2]/div/div[1]/div/div/div/div/div[3]/div[1]/div[2]/div[2]/p\\')\\n    storyStorageArray.append(element.text)\\n    text_file.write(storyStorageArray[i] + \\'\\n\\')\\n    \\ntext_file.close()\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9TmQelxvbKq",
        "colab_type": "text"
      },
      "source": [
        "####Pulling and storage of sample data into an array\n",
        "\n",
        "The sample data, which is stored in a file \"Output.txt\" in my Google Drive, is opened and its contents are stored in an array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCqjcVS1_oT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diaryEntries = []\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open(\"/content/drive/My Drive/Output.txt\", \"r\", encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "for line in lines:\n",
        "    newVar = (line)\n",
        "    diaryEntries.append(newVar)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_DPoNX2zVZv",
        "colab_type": "text"
      },
      "source": [
        "####Running of algorithm\n",
        "\n",
        "The last entry of the 84 stories pulled from the Humans of New York page is used as a test case, and the rest of the entries are used as training data. \n",
        "\n",
        "It can be seen that the algorithm works well, with the majority of the stories returned being similar in content to the first story, about a genocide in Rwanda. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pqjma8SzNSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "storyArray = []#Keeps all stories \n",
        "categoryTree = [None] * 6250 #Keeps all categories. Has category objects, which contain entity hash tables \n",
        "\n",
        "'''\n",
        "For each story, the 5 steps are performed \n",
        "'''\n",
        "for i in range(0,len(diaryEntries)):\n",
        "  \n",
        "  story = diaryEntries[i]\n",
        "  \n",
        "  #Step 1 \n",
        "  storyArray,storyPointer = storeStory(story,storyArray)  #O(1) wrt current Size\n",
        "  \n",
        "  #Step 2 \n",
        "  storyObject = classify(storyPointer) #O(1)\n",
        "  \n",
        "  #Step 3 \n",
        "  newCategory = initializeRelationships(categoryTree,storyObject) #O(1)\n",
        "  \n",
        "  #Step 4\n",
        "  returnStoryIndices = insertStoryIntoTree(newCategory,categoryTree,storyObject) \n",
        "  \n",
        "  #Step 5 \n",
        "  if len(returnStoryIndices) < 15: \n",
        "    returnStoryIndices = searchParentSiblingCategories(returnStoryIndices,newCategory,categoryTree,storyObject)\n",
        "    \n",
        "testCaseStory = diaryEntries[-1]\n",
        "\n",
        "output = []\n",
        "print(\"Input\")\n",
        "printStatement = \"Diary Entry:\" + str(testCaseStory)\n",
        "print(printStatement)\n",
        "output.append(printStatement)\n",
        "\n",
        "print(\"Output\")\n",
        "for indice in returnStoryIndices:   \n",
        "  printStatement = \"Returned Related Story:\" +str(storyArray[indice])\n",
        "  print(printStatement)\n",
        "  output.append(printStatement)\n",
        "  \n",
        "  \n",
        "with open(\"/content/drive/My Drive/FinalOutput.txt\", \"w\", encoding='utf-8') as f: \n",
        "  for outputEntry in output:\n",
        "    f.write(outputEntry)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoAcOnj5BDp",
        "colab_type": "text"
      },
      "source": [
        "##Explanation of algorithmic design "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEtIbPve5JIX",
        "colab_type": "text"
      },
      "source": [
        "###Hash table "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xz7EFFnM6Zj",
        "colab_type": "text"
      },
      "source": [
        "####Analysis <br> \n",
        "\n",
        "\n",
        "A hash table hashes the input, generates the index of the hash table where the input should be stored and stores the input in that index. In this application, hash tables were used to store categories, and within categories hash tables were used to store entities. Noteworthy is the fact that while normal hash tables allow for collisions, my implementation of hash tables simply combines the entities in order to reduce the complexity of the program. \n",
        "<br> <br> \n",
        "\n",
        "*Insertion and Access time complexity* \n",
        "\n",
        "A hash table has an average case insertion and access time complexity of O(1), as compared to an array which also has a insertion time complexity of O(1) but has an access time complexity of O(n). The complexity of the hash table depends on the effectiveness of the hash function in spreading out the data throughout the hash table, as well as the size of the hash table. If the hash table is small or if the hash functions fails to give a uniform distribution of hash table indexes, collisions would occur where the same index would have many entries stored. Open addressing is implemented, hence the function will be re-hashed to find the next available open slot. This would lead to the access time complexity approaching O(m), where $m$ is the length of the hash table, as compared to O(1) if no collisions occur. \n",
        "<br> \n",
        "\n",
        "####Implementation\n",
        "<br> \n",
        "*Hash Function*\n",
        "\n",
        "The hash function used was the Fowler-Noll-Vo hashing function. he hash function takes in the data being hashed as an argument.\n",
        "\n",
        "For each input element, it multiplies the offset_basis, which is a constant depending on the size of the hash(which is constant in this case), by FNV_prime.\n",
        "\n",
        "* Offset bias is an integer whose value depends on the size of the hash but is roughly 1.0* 10^78 for for this implementation of the bloom filter, which hashes a 36-bit alphanumeric number.\n",
        "* FNV Prime is a prime number seeded by an input number at initialization. Therefore, by seeding the hash function with different numbers at initialization, we can get distinct hash funcitons as the FNV_prime number is different.\n",
        "\n",
        "The 2 terms multiplied constitute the hash that will be used on the data.The hash is then used on the data, and the result is returned.\n",
        "<br> \n",
        "<br> \n",
        "*Determining hash table size*\n",
        "\n",
        "The formula for probability collisions is the table size $m$ divided by the number of items $n$, assuming a hash function that is effective in evenly distributing the entries. A locally run web scraper was created to find the total number of categories available in the Natural Language Processing API - 625. Only 10% of the avaliable categories are expected to be used, giving a $n$ of 62.5. With a target collision rate of 1%, this was used to arrive at the hash table array size of 6250. The number of entities is arbitrary - all $n$ stories under the same entities, leading to 16 total entities, or they could all have different entities, leading to $16n$ entities. Further analysis could help understand the optimal hash table size for entities - it is initated at 500 for now. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3v7yubr5Wka",
        "colab_type": "text"
      },
      "source": [
        "###Max Heap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie8Gzjs66dpL",
        "colab_type": "text"
      },
      "source": [
        "####Analysis \n",
        "\n",
        "A max heap was used because it was the most efficient data structure to get the maximum value for any given memory. It has a O(1) complexity to return the maximum value, and a O(logn) complexity to insert or delete any node, compared to sorted arrays which also have O(1) complexity to return the maximum value but have O(n) insertion time "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rGkznKl8-Pi",
        "colab_type": "text"
      },
      "source": [
        "####Implementation\n",
        "\n",
        "The heap class implemented (Copied below)has working push, delete and getMax methods, as demonstrated below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUeKTSeRpjdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#Commented out because it has already been implemented above, pasted here for \n",
        "#reference \n",
        "\n",
        "class maxHeap:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.salienceValues = []\n",
        "    \n",
        "    \n",
        "  def push(self, data):\n",
        "    self.salienceValues.append(data)\n",
        "    self.floatUp(len(self.salienceValues) - 1)\n",
        "    \n",
        "  def getMax(self): \n",
        "    return self.salienceValues[0]\n",
        "  \n",
        "  def delete(self,data):\n",
        "    index = self.salienceValues.index(data)\n",
        "    \n",
        "    if len(self.salienceValues) > 2:\n",
        "      self.swap(index, len(self.salienceValues) - 1)\n",
        "      returnValue = self.salienceValues.pop()\n",
        "      self.bubbleDown(index)\n",
        "    elif len(self.salienceValues) <= 2:\n",
        "      returnValue = self.salienceValues.pop()\n",
        "    return returnValue\n",
        "\n",
        "  def swap(self, i, j):\n",
        "    self.salienceValues[i], self.salienceValues[j] = self.salienceValues[j], self.salienceValues[i]\n",
        "\n",
        "  def floatUp(self, index):\n",
        "    parent = index//2\n",
        "    arrayInput = self.salienceValues\n",
        "    \n",
        "    if index <= 0: \n",
        "      return\n",
        "    elif arrayInput[index][\"salienceValue\"] > arrayInput[parent][\"salienceValue\"]:\n",
        "      self.swap(index, parent)\n",
        "      self.floatUp(parent)\n",
        "\n",
        "  def bubbleDown(self, index):\n",
        "    left = index * 2\n",
        "    right = index * 2 + 1\n",
        "    largest = index\n",
        "    if len(self.salienceValues) > left and self.salienceValues[largest][\"salienceValue\"] < self.salienceValues[left][\"salienceValue\"]:\n",
        "      largest = left\n",
        "    if len(self.salienceValues) > right and self.salienceValues[largest][\"salienceValue\"] < self.salienceValues[right][\"salienceValue\"]:\n",
        "      largest = right\n",
        "    if largest != index:\n",
        "      self.swap(index, largest)\n",
        "      self.bubbleDown(largest)\n",
        "\n",
        "'''\n",
        "\n",
        "import random \n",
        "\n",
        "sampleCategoryMaxHeap = maxHeap()\n",
        "\n",
        "'''\n",
        "PUSH Method \n",
        "\n",
        "5 items are pushed into the max heap\n",
        "'''\n",
        "print(\"PUSH\")\n",
        "print(\"Before:\",sampleCategoryMaxHeap.salienceValues )\n",
        "for i in range(5): \n",
        "  dict = {}\n",
        "  dict[\"Name\"] = i\n",
        "  dict[\"index\"] = i \n",
        "  dict[\"salienceValue\"] = random.randint(1,40)\n",
        "  sampleCategoryMaxHeap.push(dict) \n",
        "print(\"After:\", sampleCategoryMaxHeap.salienceValues,\"\\n\")\n",
        "\n",
        "'''\n",
        "GET MAXIMUM Method \n",
        "\n",
        "The tuple with the maximum salience Value is returned \n",
        "'''\n",
        "print(\"MAX VALUE\")\n",
        "maxValue = sampleCategoryMaxHeap.getMax()\n",
        "print(\"Maximum:\", maxValue,\"\\n\")\n",
        "\n",
        "'''\n",
        "DELETE ELEMENT Method\n",
        "\n",
        "An element is deleted from the tree\n",
        "'''\n",
        "print(\"DELETE\")\n",
        "print(\"Before:\",sampleCategoryMaxHeap.salienceValues )\n",
        "sampleCategoryMaxHeap.delete(dict)\n",
        "print(\"After:\", sampleCategoryMaxHeap.salienceValues)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U89Ni2oP9hl2",
        "colab_type": "text"
      },
      "source": [
        "####Performance \n",
        "\n",
        "First, the performance was analyzed for the insert operation. Building a heap takes nlogn time using a top-down heapify method. However, this program utilizes a bottom-up heapify method where each element is inserted into a almost sorted existing max-heap. \n",
        "\n",
        "The heap with $n$ nodes has has $n/2$ leaf nodes that cannot be swopped down. At the next level, it has $n/4$ nodes that can be moved down at most once, thus querying the swap function twice. This progresses on till we get the Geometric progression shown below, where the term of the highest order is n and therefore the overall complexity is O(n) (Wilson, 2012)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYu-XBA67Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "uploaded = files.upload()\n",
        "Image(\"heapEqn.png\", width=600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdDfcRdh9l_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Insert Operation\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "\n",
        "performanceMeasurementMaxHeap = maxHeap()\n",
        "timingData = [] \n",
        "xValues = []\n",
        "complexityValues = []\n",
        "\n",
        "for i in range(1,100):\n",
        "  for k in range(1,10): \n",
        "    averageTimer = []\n",
        "    start_time = time.time()\n",
        "    for j in range(1,i*10): \n",
        "      dict = {}\n",
        "      dict[\"Name\"] = i\n",
        "      dict[\"index\"] = i \n",
        "      dict[\"salienceValue\"] = random.randint(1,40)\n",
        "      performanceMeasurementMaxHeap.push(dict) \n",
        "    averageTimer.append((time.time() - start_time))\n",
        "    \n",
        "  timingData.append((sum(averageTimer)/float(len(averageTimer))))\n",
        "  xValues.append(i)\n",
        "  complexityValues.append(i)\n",
        "  \n",
        "lowestI = timingData[0]\n",
        "timingData[:] = [x / lowestI for x in timingData]\n",
        "\n",
        "plt.plot(xValues, timingData, label = \"Insertion Operation\" )\n",
        "plt.plot(xValues, complexityValues, label = \"n\" )\n",
        "plt.legend()\n",
        "plt.xlabel('Input Size')\n",
        "plt.ylabel('Time taken, common sized')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUwiAMXPLojw",
        "colab_type": "text"
      },
      "source": [
        "Analyzing the performance for the deletion operation, we can see it takes O(n(logn)^2). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZA03FVxEMVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Deletion Operation\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "\n",
        "DeletionPerformanceMeasurementMaxHeap = maxHeap()\n",
        "timingData = [] \n",
        "xValues = []\n",
        "complexityValues = []\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1,100):\n",
        "  \n",
        "  for j in range(1,10): \n",
        "    averageTimer = []\n",
        "    start_time = time.time()\n",
        "    for k in range(0,i): \n",
        "      dict = {}\n",
        "      dict[\"Name\"] = k\n",
        "      dict[\"index\"] = k\n",
        "      dict[\"salienceValue\"] = random.randint(1,40)\n",
        "      DeletionPerformanceMeasurementMaxHeap.push(dict) \n",
        "\n",
        "    start_time = time.time()\n",
        "    DeletionPerformanceMeasurementMaxHeap.delete(dict) \n",
        "    averageTimer.append((time.time() - start_time))\n",
        "    \n",
        "  timingData.append((sum(averageTimer)/float(len(averageTimer))))\n",
        "  xValues.append(i)\n",
        "  complexityValues.append(i*math.pow(math.log(i),2))\n",
        "lowestI = timingData[0]\n",
        "timingData[:] = [x / lowestI for x in timingData]\n",
        "\n",
        "plt.plot(xValues, timingData, label = \"Deletion Operation\" )\n",
        "plt.plot(xValues, complexityValues, label = \"n * (logn)^2 \" )\n",
        "plt.legend()\n",
        "plt.xlabel('Input Size')\n",
        "plt.ylabel('Time taken, common sized')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ohQFdXMa-V",
        "colab_type": "text"
      },
      "source": [
        "Analyzing the performance for the getMax operation, we can see it has complexity of about O(1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dUvluh6EJJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get Max Operation\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "getMaxPerformanceMeasurementMaxHeap = maxHeap()\n",
        "timingData = [] \n",
        "xValues = []\n",
        "complexityValues = []\n",
        "\n",
        "for i in range(1,100):\n",
        "  for k in range(1,10): \n",
        "    averageTimer = []\n",
        "    start_time = time.time()\n",
        "    for j in range(0,i): \n",
        "      dict = {}\n",
        "      dict[\"Name\"] = j\n",
        "      dict[\"index\"] = j\n",
        "      dict[\"salienceValue\"] = random.randint(1,40)\n",
        "      getMaxPerformanceMeasurementMaxHeap.push(dict) \n",
        "\n",
        "    start_time = time.time()\n",
        "    getMaxPerformanceMeasurementMaxHeap.getMax\n",
        "    averageTimer.append((time.time() - start_time))\n",
        "    \n",
        "  timingData.append((sum(averageTimer)/float(len(averageTimer))))\n",
        "  xValues.append(i)\n",
        "  complexityValues.append(1)\n",
        "\n",
        "lowestI = timingData[0]\n",
        "timingData[:] = [x / lowestI for x in timingData]\n",
        "\n",
        "plt.plot(xValues, timingData, label = \"Get Max Operation\" )\n",
        "plt.plot(xValues, complexityValues, label = \"O(1)\" )\n",
        "plt.legend()\n",
        "plt.xlabel('Input Size')\n",
        "plt.ylabel('Time taken, common sized')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b42I3BcK5wG-",
        "colab_type": "text"
      },
      "source": [
        "###Overall Complexity Analysis and performance\n",
        "\n",
        "There are 3 main opeartions - 1) hashing and insertion of category into the category tree hashtable, 2) hashing to find the correct entities in the entities hash table 3) Withdrawing stories with similar entities from the max heap at the given position of the entities hash table, and inserting the entities of the story into the max heap. \n",
        "\n",
        "The operations with regards to the hash tables have O(1) average time complexity, and O(n) worst case complexity. The insertion and getMax() operation, on the other hand, have O(n) and O(1) average complexities, as shown earlier in the simulations, and have O(nlogn) and O(1) worst case complexities . \n",
        "\n",
        "Although the 3 operations are nested, each of them is only performed n times per story. For example, although it takes O(n) worst-case complexity to find the correct category index in the CategoryTree hashtable and within the category it takes O(n) worst-case complexity to find the correct category index in the Entiteis hashtable, the latter operation is only performed once. Therefore, to find the overall complexity, we do not multiply the complexities but instead add them. The overall average case complexity of the operations is therefore O(n), taking the largest of the average complexities of the 3 operations, and the overall worst-case complexity is O(nlogn), taking the largest of the worst complexities of the 3 operations. Complexities of operations such as delete were explored but are not expected to run frequently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9We41fqi2B9d",
        "colab_type": "text"
      },
      "source": [
        "###Conclusion\n",
        "\n",
        "This paper, using hash tables, trees and max-heaps in conjunction with the Google Natural Language API, has provided a preliminary way in which users can discover content through self.expression. The test set worked well, with a large majority of the stories returned directly relevant to the story used as input. \n",
        "\n",
        "However, to extend this project, there should be more data to build a better and more customized training model. Diary-writing tends to have texts that are centered around emotional topics, and the majority of topics that are under Google's Natural Language API are irrelevant. However, these categories can be used to encourage self-expression beyond emotional self-expression, in the form of intellectual self-expression. It would be best that for each of these fields, a new model is trained to give more precise categories, so as to increase the accuracy and relevance of the posts returned, as well as to reduce runtime, which would be longer if many stories are clumped together in few categories and hence the height of the max heaps in each of these categories is high, leading to longer insertion times. \n",
        "\n",
        "Lastly, the input to this algorithm - text- can be created in other ways other than the user manually writing them. Machine learning and big data pulled from users' online habits, with their explicit consent, could be used to create a picture of their current circumstances, and this could be converted into text that could be fed into the program. I intend to do continue this part of the project in future courses, to build towards a capstone where I create a new model of content discovery where the content is meaningful and touching. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwMurv0U38eA",
        "colab_type": "text"
      },
      "source": [
        " ###HCs \n",
        " \n",
        "  #distributions - A distribution of the performance of various components of the algorithm at different inout sizes is created, compared to the theoretical estimate as well as discussed \n",
        "  \n",
        "  #probabiity - Probability of collisions was applied  as a variable to come up with the ideal hash table size\n",
        "  \n",
        "  #audience - The target end user - social-media-savvy youth, is studied carefully to come up with a algorithm that can best add value to their lives. \n",
        "\n",
        "###LOs\n",
        "\n",
        " #novelapplication - A novel combination of the Google API, heaps, trees and hash tables was used to create a working prototype to solve an interesting social problem\n",
        " \n",
        "  #optimalalgorithm - The merits of using particular data structures was discussed in depth, and the effectiveness of these choices was shown with simulations and theoretical analysis when relevant. \n",
        "  \n",
        "  #complexity - The theoretical complexity of various stages of the algorithm, as well as the algorithm as a whole, is discussed, the underlying reasons are explained and the complexity is tested through simulations. \n",
        "  \n",
        "  #hashing - A working hash function is implemeneted, and the benefits and drawbacks of using hash functions are discussed. Furthermore, the reasons behind the use of hash functions,  their mechanisms as well as their performance are explored. Open addressing is implemented where necessary and explained. \n",
        "  \n",
        "  #searchtrees -   A working max heap implementation that stores objects is is provided, and its benefits and drawbacks are discussed. Furthermore, a discussion of the reasons behind the use of max heaps, the benefits and drawbacks as well as a simulation of its performance is conducted\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVNTAGbTf_Vx",
        "colab_type": "text"
      },
      "source": [
        "###References\n",
        "\n",
        "Wilson, T. (2012, October 1). Inserting an element in a heap takes O(log n). Still if we insert n elements in the heap it comes out to be O(n)? Retrieved December 21, 2018, from https://www.quora.com/Inserting-an-element-in-a-heap-takes-O-log-n-Still-if-we-insert-n-elements-in-the-heap-it-comes-out-to-be-O-n"
      ]
    }
  ]
}
